# Strata v0 – Engineering Design Document

**Version:** 0.1
**Target:** v0 design-partner build
**Scope:** Single SMB connector, single Postgres instance, single deployment environment (dev/staging)

---

## 1. Goals & Non-Goals

### 1.1 Goals (v0)

1. **Ingest SMB shares via a Linux agent**

   * Enumerate files on one or more SMB shares.
   * Capture file metadata and ACLs.
   * Send “file discovered / modified / deleted / ACL changed” events to a central API.

2. **Index text-like documents**

   * File types: `PDF`, `DOCX`, `TXT`, optionally `PPTX`.
   * Extract text content, normalize to `Document` + `Chunk` model.
   * Store in Postgres and (optionally) pgvector for embeddings.

3. **Detect sensitive content & compute exposure**

   * PII-like patterns (emails, phone numbers, possible SSNs, possible credit card numbers, basic secrets tokens).
   * Compute exposure level per document based on ACL breadth:

     * `LOW`, `MEDIUM`, `HIGH`.

4. **Provide a minimal UI + API**

   * Dashboard: counts of sensitive documents by type and exposure.
   * Table of findings: path, sensitivity types, exposure.
   * Programmatic API:

     * `POST /v0/sensitivity/find` (aka `find_sensitive_content`).

5. **Enforce per-tenant isolation and ACLs**

   * Multi-tenant DB with `tenant_id` in all main tables.
   * Every search/finding query filtered by both `tenant_id` and caller’s effective principals.

### 1.2 Non-Goals (v0)

* No write operations to customer file systems (no ACL changes, no file moves).
* No SharePoint/OneDrive/Box connectors.
* No remediation APIs (`apply_remediation`).
* No OCR of images/scanned PDFs.
* No complex agent orchestration beyond a clean API that agents can call.

---

## 2. High-Level Architecture

### 2.1 Components

1. **On-Prem SMB Connector Agent (Python)**

   * Runs in customer environment (VM/container).
   * Mounts one or more SMB shares read-only.
   * Periodically scans directories; computes metadata + ACLs.
   * Sends batched events to cloud Strata API.

2. **Strata Control Plane (Python / FastAPI)**

   * HTTP API for:

     * Ingestion events from connectors.
     * Internal worker APIs (optional).
     * Query APIs for UI/agents.
   * Manages tenants, principals, shares, files, documents, chunks, sensitivities, exposure, and jobs.

3. **Worker Services (Python)**

   * Decoupled from API; poll DB for jobs.
   * Services:

     * **Content Extraction Worker**
     * **Normalization & Chunking Worker**
     * **Enrichment Worker** (embeddings + sensitivity + exposure)

4. **Data Stores**

   * **Postgres** (core relational store; pgvector extension enabled).
   * **Object Storage (local filesystem for v0)** for “raw artifacts” / extracted text if needed (optional — for v0, you may keep everything in Postgres).

5. **Web UI (React / Next.js or SPA)**

   * Calls Strata API via access token.
   * Provides:

     * Overview dashboard.
     * Findings table with filters.

### 2.2 Logical Data Flow

1. SMB Agent scans share → sends `file_*` events → Strata Ingestion API.
2. Ingestion API normalizes events → upserts `File`, `FileAcl`, `FileEvent`, creates `Job` for new/changed files.
3. Content Extraction Worker processes `EXTRACT_CONTENT` jobs → reads file (via SMB path), extracts text → inserts `Document` + `Chunk` rows → creates `ENRICH_CHUNKS` job.
4. Enrichment Worker:

   * Computes embeddings (optional).
   * Runs sensitivity detectors → populates `SensitivityFinding`.
   * Computes `ExposureScore` per document.
5. UI/API queries:

   * `find_sensitive_content` filters by tenant, scope (path prefix), sensitivity types, exposure levels, and caller’s effective principals.

---

## 3. Technology Choices

* **Backend:** Python, FastAPI.
* **Agent:** Python (runs on Linux; uses `mount.cifs` + `os.walk`).
* **DB:** Postgres with pgvector extension.
* **Queue:** Simple DB-backed job table; workers poll with `SELECT … FOR UPDATE SKIP LOCKED`.
* **Auth:** Static API keys per tenant (Bearer token) for v0.
* **Embeddings:** Optional in v0; if enabled, use OpenAI embeddings via environment-provided API key.

---

## 4. Data Model (Postgres)

Below is a concrete schema the coding agent can implement. Use UUIDs as `uuid` type.

### 4.1 Core Multi-Tenancy

```sql
CREATE TABLE tenant (
  id           uuid PRIMARY KEY,
  name         text NOT NULL,
  created_at   timestamptz NOT NULL DEFAULT now(),
  api_key_hash text NOT NULL         -- bcrypt/argon2 hash of API key
);
```

### 4.2 Estates, Shares, Files

```sql
CREATE TABLE estate (
  id         uuid PRIMARY KEY,
  tenant_id  uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  name       text NOT NULL,
  created_at timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE share (
  id           uuid PRIMARY KEY,
  tenant_id    uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  estate_id    uuid NOT NULL REFERENCES estate(id) ON DELETE CASCADE,
  name         text NOT NULL,          -- logical name (e.g. HRShare)
  share_type   text NOT NULL,          -- 'SMB'
  root_path    text NOT NULL,          -- e.g. '\\server\share' or mount path
  created_at   timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE file (
  id             uuid PRIMARY KEY,
  tenant_id      uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  share_id       uuid NOT NULL REFERENCES share(id) ON DELETE CASCADE,
  relative_path  text NOT NULL,       -- path relative to share root
  name           text NOT NULL,       -- basename
  size_bytes     bigint NOT NULL,
  mtime          timestamptz NOT NULL,
  file_type      text NOT NULL,       -- mime-type or simple enum
  content_hash   text NOT NULL,       -- SHA256 of content (hex)
  acl_hash       text NOT NULL,       -- hash of ACL for change detection
  last_seen_at   timestamptz NOT NULL,
  deleted        boolean NOT NULL DEFAULT false,
  created_at     timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, share_id, relative_path)
);
```

### 4.3 Principals, Groups, ACLs

```sql
CREATE TABLE principal (
  id           uuid PRIMARY KEY,
  tenant_id    uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  type         text NOT NULL,         -- 'USER' | 'GROUP' | 'SERVICE'
  external_id  text NOT NULL,         -- AD SID or other ID
  display_name text NOT NULL,
  created_at   timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, external_id)
);

CREATE TABLE group_membership (
  id                  uuid PRIMARY KEY,
  tenant_id           uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  group_id            uuid NOT NULL REFERENCES principal(id) ON DELETE CASCADE,
  member_principal_id uuid NOT NULL REFERENCES principal(id) ON DELETE CASCADE,
  created_at          timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, group_id, member_principal_id)
);

-- Raw ACL entries as received from connector
CREATE TABLE file_acl_entry (
  id                uuid PRIMARY KEY,
  tenant_id         uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  file_id           uuid NOT NULL REFERENCES file(id) ON DELETE CASCADE,
  principal_id      uuid NOT NULL REFERENCES principal(id),
  rights            text NOT NULL,        -- e.g. 'R', 'RW', 'FULL'
  source            text NOT NULL,        -- 'FILE' | 'INHERITED'
  created_at        timestamptz NOT NULL DEFAULT now()
);

-- Effective access compressed by principal/group for fast queries
CREATE TABLE file_effective_access (
  id             uuid PRIMARY KEY,
  tenant_id      uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  file_id        uuid NOT NULL REFERENCES file(id) ON DELETE CASCADE,
  principal_id   uuid NOT NULL REFERENCES principal(id),
  can_read       boolean NOT NULL,
  created_at     timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, file_id, principal_id)
);
```

### 4.4 Documents, Chunks, Embeddings

```sql
CREATE TABLE document (
  id             uuid PRIMARY KEY,
  tenant_id      uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  file_id        uuid NOT NULL REFERENCES file(id) ON DELETE CASCADE,
  title          text NOT NULL,
  file_type      text NOT NULL,
  size_bytes     bigint NOT NULL,
  last_indexed_at timestamptz NOT NULL,
  content_hash   text NOT NULL,
  created_at     timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE chunk (
  id             uuid PRIMARY KEY,
  tenant_id      uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  document_id    uuid NOT NULL REFERENCES document(id) ON DELETE CASCADE,
  chunk_index    integer NOT NULL,       -- 0-based order
  section_heading text,                  -- nullable
  text           text NOT NULL,
  char_start     integer NOT NULL,
  char_end       integer NOT NULL,
  created_at     timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, document_id, chunk_index)
);

-- Requires pgvector extension
CREATE TABLE chunk_embedding (
  chunk_id    uuid PRIMARY KEY REFERENCES chunk(id) ON DELETE CASCADE,
  tenant_id   uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  embedding   vector(1536),              -- adjust dim as needed
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

### 4.5 Sensitivity & Exposure

```sql
CREATE TYPE sensitivity_type AS ENUM (
  'PERSONAL_DATA',
  'HEALTH_DATA',
  'FINANCIAL_DATA',
  'SECRETS',
  'OTHER'
);

CREATE TYPE sensitivity_level AS ENUM ('LOW', 'MEDIUM', 'HIGH');
CREATE TYPE exposure_level AS ENUM ('LOW', 'MEDIUM', 'HIGH');

CREATE TABLE sensitivity_finding (
  id               uuid PRIMARY KEY,
  tenant_id        uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  document_id      uuid NOT NULL REFERENCES document(id) ON DELETE CASCADE,
  chunk_id         uuid REFERENCES chunk(id) ON DELETE CASCADE,
  sensitivity_type sensitivity_type NOT NULL,
  sensitivity_level sensitivity_level NOT NULL,
  snippet          text NOT NULL,
  created_at       timestamptz NOT NULL DEFAULT now()
);

CREATE TABLE document_exposure (
  id               uuid PRIMARY KEY,
  tenant_id        uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  document_id      uuid NOT NULL REFERENCES document(id) ON DELETE CASCADE,
  exposure_level   exposure_level NOT NULL,
  exposure_score   integer NOT NULL CHECK (exposure_score BETWEEN 0 AND 100),
  access_summary   jsonb NOT NULL,  -- e.g. { "broad_groups": [...], "user_count_bucket": "100-1000" }
  created_at       timestamptz NOT NULL DEFAULT now(),
  UNIQUE (tenant_id, document_id)
);
```

### 4.6 Jobs & Events

```sql
CREATE TYPE job_type AS ENUM (
  'EXTRACT_CONTENT',
  'ENRICH_CHUNKS'
);

CREATE TYPE job_status AS ENUM (
  'PENDING',
  'IN_PROGRESS',
  'SUCCEEDED',
  'FAILED'
);

CREATE TABLE job (
  id           uuid PRIMARY KEY,
  tenant_id    uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  job_type     job_type NOT NULL,
  file_id      uuid REFERENCES file(id),
  document_id  uuid REFERENCES document(id),
  status       job_status NOT NULL DEFAULT 'PENDING',
  attempts     integer NOT NULL DEFAULT 0,
  last_error   text,
  created_at   timestamptz NOT NULL DEFAULT now(),
  updated_at   timestamptz NOT NULL DEFAULT now()
);

CREATE TYPE file_event_type AS ENUM (
  'FILE_DISCOVERED',
  'FILE_MODIFIED',
  'FILE_DELETED',
  'ACL_CHANGED'
);

CREATE TABLE file_event (
  id           uuid PRIMARY KEY,
  tenant_id    uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  file_id      uuid REFERENCES file(id),
  share_id     uuid REFERENCES share(id),
  event_type   file_event_type NOT NULL,
  payload      jsonb NOT NULL,
  created_at   timestamptz NOT NULL DEFAULT now()
);
```

---

## 5. On-Prem SMB Connector Agent

### 5.1 Responsibilities

* Mount SMB share(s) read-only.
* Perform **initial full scan** and **periodic incremental scans**.
* For each file:

  * Collect path, size, mtime, type.
  * Compute content hash (SHA256) if configured.
  * Retrieve ACL entries (SIDs and rights).
* Emit events to Strata Ingestion API in batches.

### 5.2 Configuration

Example YAML:

```yaml
agent_id: "agent-uuid-or-name"
tenant_api_key: "<redacted>"
api_base_url: "https://api.strata.example.com"
scan_interval_seconds: 600

shares:
  - name: "HRShare"
    smb_uri: "\\\\server\\hr"
    mount_point: "/mnt/hrshare"
    include_paths:
      - "/"
    exclude_patterns:
      - "*.tmp"
      - "~*"
    max_file_size_bytes: 104857600  # 100MB
```

Environment variables:

* `STRATA_API_KEY`
* `STRATA_API_BASE_URL`

### 5.3 Event Payload

The agent calls:

`POST /v0/ingest/events`

Request body:

```json
{
  "agent_id": "agent-123",
  "events": [
    {
      "type": "FILE_DISCOVERED",
      "share_name": "HRShare",
      "relative_path": "Finance/Payroll/Q4/bonus_plan.docx",
      "size_bytes": 123456,
      "mtime": "2025-12-08T18:00:00Z",
      "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "content_hash": "sha256:...",
      "acl_hash": "sha256:...",
      "acl_entries": [
        {
          "principal_external_id": "S-1-5-21-...",
          "rights": "R",
          "source": "FILE"
        }
      ]
    }
  ]
}
```

`FILE_MODIFIED`, `FILE_DELETED`, `ACL_CHANGED` use the same structure with only relevant fields.

### 5.4 Implementation Notes (Agent)

* Use OS-level `mount.cifs` to mount SMB; the agent just walks the mount.
* Use `os.walk()` to traverse directories.
* Use `hashlib.sha256` on file contents (streamed) to compute content hash.
* For v0, you can simplify ACL retrieval to:

  * A placeholder mapping (if true SID retrieval is deferred).
  * Or call `getfacl` / extended attributes if available with SIDs.
* The agent must be idempotent: scanning re-sends events that the backend can dedupe via `(share_id, relative_path, content_hash, acl_hash)`.

---

## 6. Ingestion API & Deduplication

### 6.1 Endpoint

`POST /v0/ingest/events`

* Auth: `Authorization: Bearer <tenant_api_key>`
* Resolves `tenant_id` from API key.

Request body: as defined above.

### 6.2 Ingestion Logic

For each event:

1. Resolve `share` by `(tenant_id, share_name)`. If missing, 400 or create once from configuration.

2. Lookup existing `file` by `(tenant_id, share_id, relative_path)`.

3. If `FILE_DELETED`:

   * Mark `file.deleted = true`, `file.last_seen_at = now()`.
   * Insert `file_event`.
   * No jobs created.

4. For `FILE_DISCOVERED` or `FILE_MODIFIED`:

   * If no file exists:

     * Insert new `file` row with metadata and hashes.
     * Insert `file_acl_entry` rows (create principals/groups as needed via `external_id`).
     * Compute initial `file_effective_access` (simple: grant read to all referenced principals).
     * Create `job` of type `EXTRACT_CONTENT` for this file.
   * If file exists:

     * Compare `content_hash` and `acl_hash`:

       * If both unchanged:

         * Update `last_seen_at`, insert `file_event`, no jobs.
       * If `content_hash` changed:

         * Update metadata and hashes.
         * Insert `file_event`.
         * Create new `EXTRACT_CONTENT` job.
       * If only `acl_hash` changed:

         * Update `acl_hash`.
         * Replace `file_acl_entry` for that file.
         * Recompute `file_effective_access`.
         * Insert `file_event`.
         * No content extraction job; we will recompute exposure during enrichment.

---

## 7. Job Processing

### 7.1 Job Claiming Pattern

Workers use SQL:

```sql
UPDATE job
SET status = 'IN_PROGRESS',
    attempts = attempts + 1,
    updated_at = now()
WHERE id = (
  SELECT id
  FROM job
  WHERE status = 'PENDING'
  ORDER BY created_at
  FOR UPDATE SKIP LOCKED
  LIMIT 1
)
RETURNING *;
```

If no row is returned, worker sleeps then retries.

### 7.2 Job Types

1. `EXTRACT_CONTENT`

   * Input: `file_id`.
   * Worker steps:

     1. Load `file` and `share`.
     2. Compute full path: `share.root_path + '/' + file.relative_path`.
     3. Open file from local filesystem (SMB mount).
     4. Extract text and simple structure based on `file_type`.
     5. Insert or update `document`.
     6. Delete existing chunks for that document; re-chunk.
     7. Insert `chunk` rows.
     8. Create `ENRICH_CHUNKS` job for `document_id`.

2. `ENRICH_CHUNKS`

   * Input: `document_id`.
   * Worker steps:

     1. Load all `chunk` rows for that `document_id`.
     2. If embeddings enabled:

        * For each chunk, call embeddings API, insert/update `chunk_embedding`.
     3. Run sensitivity detection over chunks.
     4. Insert `sensitivity_finding` rows.
     5. Compute `document_exposure` based on:

        * Sensitivity levels/types.
        * Breadth of `file_effective_access`.
     6. Mark job `SUCCEEDED`.

---

## 8. Content Extraction & Chunking

### 8.1 Extraction Service Interface

Within the worker, define an abstraction:

```python
class ExtractedDocument(BaseModel):
    title: str
    text: str         # full text
    sections: list[dict]  # optional, for future use


def extract_content(path: str, file_type: str) -> ExtractedDocument:
    ...
```

Implementation notes:

* For `text/plain`: read file as UTF-8 with fallback.
* For `PDF`: use pdfminer.six or similar.
* For `DOCX`: use python-docx.
* For `PPTX`: use python-pptx (optional in v0).

If extraction fails, mark job `FAILED` and store `last_error`.

### 8.2 Chunking Strategy

Simple fixed-size chunking for v0:

* Normalize whitespace in `text`.
* Choose `CHUNK_SIZE = 1000` characters, `CHUNK_OVERLAP = 200` characters.
* Loop over `text` and create chunks with index, `char_start`, `char_end`.

Pseudocode:

```python
def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[ChunkSpec]:
    chunks = []
    start = 0
    index = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunk_text = text[start:end]
        chunks.append(ChunkSpec(index=index, text=chunk_text, char_start=start, char_end=end))
        index += 1
        start = end - overlap
        if start < 0:
            start = 0
    return chunks
```

---

## 9. Enrichment: Embeddings, Sensitivity, Exposure

### 9.1 Embeddings (Optional in v0)

If `ENABLE_EMBEDDINGS=true`:

* For each `chunk`, call embedding provider:

```python
def embed_chunks(chunks: list[str]) -> list[list[float]]:
    # Use OpenAI embeddings or similar
    ...
```

* Store in `chunk_embedding` table.

### 9.2 Sensitivity Detection (Regex-based v0)

For each chunk text:

* Apply pattern detectors:

  * Emails: `\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b`
  * Phone numbers: loose pattern for `(xxx) xxx-xxxx`, `xxx-xxx-xxxx`.
  * SSN-like: `\b\d{3}-\d{2}-\d{4}\b` (flag as possible; not authoritative).
  * Credit card-like: 13–19 digit sequences with Luhn check.
  * Secrets: patterns like `AKIA[0-9A-Z]{16}`, `(?i)api_key`, etc.

For each match, create `sensitivity_finding` row:

* `sensitivity_type`:

  * Emails/phone/SSN → `PERSONAL_DATA`.
  * Credit card → `FINANCIAL_DATA`.
  * Secrets → `SECRETS`.
* `sensitivity_level`:

  * Use heuristic: `HIGH` for credit-card-like and secrets, else `MEDIUM`.

Store a small snippet (e.g., ±50 chars around match) in `snippet`.

### 9.3 Exposure Computation

We approximate exposure based on breadth of principals with read access and their nature.

For each `document`:

1. Get `file_id`.

2. Count principals with read access from `file_effective_access`:

   ```sql
   SELECT COUNT(*) FROM file_effective_access
   WHERE tenant_id = :tenant_id
     AND file_id = :file_id
     AND can_read = true;
   ```

3. Compute `principal_breadth_score`:

   * `0–10` principals → score 20
   * `11–100` → score 50
   * `>100` → score 80

4. Check if any principals correspond to “broad” groups:

   * e.g., group display_name in `('Domain Users', 'All Employees', 'Everyone')`
   * If yes, add +20.

5. Compute `sensitivity_score` from findings:

   * If any `SECRETS` or `FINANCIAL_DATA` with `HIGH` → 80
   * Else if any `PERSONAL_DATA` → 60
   * Else → 20

6. Final exposure score:

   ```text
   exposure_score = min(100, sensitivity_score + principal_breadth_score)
   ```

7. Derive `exposure_level`:

   * `0–39` → `LOW`
   * `40–69` → `MEDIUM`
   * `70–100` → `HIGH`

8. Build `access_summary` JSON:

   * `broad_groups`: list of broad group names found.
   * `principal_count_bucket`: `"0-10"`, `"11-100"`, `">100"`.

Insert/update `document_exposure`.

---

## 10. Identity & ACL Evaluation at Query Time

### 10.1 Principal Resolution

For each query request from UI/API:

1. Caller is authenticated as `principal_id` (for v0, you may hard-code one per tenant or encode in token).

2. Resolve group memberships:

   ```sql
   SELECT gm.group_id
   FROM group_membership gm
   WHERE gm.tenant_id = :tenant_id
     AND gm.member_principal_id = :principal_id;
   ```

3. Build `authorized_principal_ids` set: `{principal_id} ∪ group_ids`.

### 10.2 ACL Filter

When querying documents, filter by:

```sql
EXISTS (
  SELECT 1
  FROM file_effective_access fea
  WHERE fea.tenant_id = d.tenant_id
    AND fea.file_id = d.file_id
    AND fea.can_read = true
    AND fea.principal_id = ANY(:authorized_principal_ids)
)
```

This ensures the caller only sees documents they can read.

---

## 11. External API Surface (for UI & Agents)

Base path: `/v0`

All endpoints:

* Require `Authorization: Bearer <tenant_api_key>` for v0.
* Use `tenant_id` derived from API key.
* Principal for ACL evaluation may be fixed for v0 (e.g., “SecurityUser”).

### 11.1 `POST /v0/sensitivity/find` (find_sensitive_content)

Request:

```json
{
  "scope": {
    "share_id": "uuid-optional",
    "path_prefix": "Finance/Q4"   // optional, relative path prefix
  },
  "sensitivity_types": ["PERSONAL_DATA", "FINANCIAL_DATA"],
  "exposure_levels": ["MEDIUM", "HIGH"],
  "page": 1,
  "page_size": 50
}
```

Response:

```json
{
  "items": [
    {
      "document_id": "uuid",
      "file_id": "uuid",
      "share_id": "uuid",
      "relative_path": "Finance/Q4/bonus_plan.docx",
      "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
      "sensitivity_summary": {
        "PERSONAL_DATA": 12,
        "FINANCIAL_DATA": 2
      },
      "exposure_level": "HIGH",
      "exposure_score": 92,
      "access_summary": {
        "broad_groups": ["Domain Users"],
        "principal_count_bucket": ">100"
      }
    }
  ],
  "page": 1,
  "page_size": 50,
  "total": 123
}
```

### 11.2 `POST /v0/search/chunks` (optional but recommended)

Request:

```json
{
  "query": "Where do we store Social Security numbers for employees?",
  "scope": {
    "share_id": "uuid-optional",
    "path_prefix": "HR"
  },
  "k": 20
}
```

Response:

```json
{
  "results": [
    {
      "chunk_id": "uuid",
      "document_id": "uuid",
      "file_id": "uuid",
      "relative_path": "HR/EmployeeData/ssn_list.txt",
      "snippet": "Employee SSN: 123-45-6789 ...",
      "score": 0.89
    }
  ]
}
```

For v0, this can be pure full-text search on `chunk.text`. Embeddings-based retrieval can be wired later.

### 11.3 Minimal Tenant/Share Management APIs (internal only)

For v0, tenant creation and share registration can be CLI-only or internal scripts. No public endpoints required, but you may define:

* `POST /v0/admin/tenant`
* `POST /v0/admin/share`

---

## 12. Web UI (v0)

### 12.1 Pages

1. **Dashboard (`/`)**

   * Cards:

     * Total files indexed.
     * Total documents with sensitivity findings.
     * Count of `HIGH` exposure documents.
   * Chart:

     * Bar chart: count of documents by `exposure_level`.
     * Pie or bar: count by `sensitivity_type`.

2. **Findings (`/findings`)**

   * Table with:

     * Path (share + relative_path).
     * Sensitivity types (badges).
     * Exposure level.
     * Exposure score.
   * Filters:

     * Share selector.
     * Path prefix text filter.
     * Sensitivity type multi-select.
     * Exposure level multi-select.

3. **Document Detail (`/documents/:id`)**

   * Show:

     * File path, type.
     * Exposure info.
     * List of sensitivity findings (snippets).
     * Optional preview of extracted text.

### 12.2 UI Implementation

* React + any component library.
* API client functions for:

  * `fetchDashboardMetrics()`
  * `findSensitiveContent(filters)`
  * `fetchDocumentDetail(id)`

---

## 13. Security, Logging, and Observability

### 13.1 Security

* All requests must use HTTPS in production.
* API key hashes stored using bcrypt or argon2.
* Every table with tenant data must include `tenant_id` and every query must filter by it.
* For v0, no row-level security in Postgres is required, but you may enable it later.

### 13.2 Logging

* Backend logs:

  * Request logs (path, status, latency).
  * Job processing logs (job id, type, status).
  * Errors with full stack traces.
* Optionally store audit logs for queries:

```sql
CREATE TABLE query_log (
  id          uuid PRIMARY KEY,
  tenant_id   uuid NOT NULL REFERENCES tenant(id) ON DELETE CASCADE,
  principal_id uuid,
  endpoint    text NOT NULL,
  parameters  jsonb NOT NULL,
  created_at  timestamptz NOT NULL DEFAULT now()
);
```

---

## 14. Implementation Plan (for the Coding Agent)

### Phase 1 – Repo & DB Setup

1. Create backend repo structure:

   * `app/main.py` (FastAPI entrypoint).
   * `app/db.py` (SQLAlchemy / asyncpg connection).
   * `app/models.py` (ORM models or plain SQL).
   * `app/api` (routers).
   * `app/workers` (job workers).
2. Create migration system (Alembic).
3. Implement all schema in Section 4 with migrations.
4. Enable pgvector extension.

### Phase 2 – Auth & Tenant Bootstrapping

1. Implement API key authentication middleware:

   * Lookup `tenant` by `api_key_hash`.
2. Implement minimal CLI or script to create a tenant and print an API key.

### Phase 3 – Ingestion API

1. Implement `/v0/ingest/events` endpoint.
2. Implement logic described in Section 6:

   * Upsert `file`.
   * Manage `file_acl_entry`, `file_effective_access`.
   * Create `job` for `EXTRACT_CONTENT` when needed.
   * Insert `file_event`.

### Phase 4 – Job Workers & Content Extraction

1. Implement generic job worker using `job` table and `SKIP LOCKED`.
2. Implement `EXTRACT_CONTENT` worker logic:

   * For a file path, call `extract_content`.
   * Insert/update `document`.
   * Chunk text and insert `chunk` rows.
   * Create `ENRICH_CHUNKS` job.

### Phase 5 – Enrichment Worker

1. Implement `ENRICH_CHUNKS` worker:

   * Load chunks, compute embeddings (optional).
   * Run sensitivity detectors.
   * Insert `sensitivity_finding` rows.
   * Compute `document_exposure`.

### Phase 6 – Query API & UI

1. Implement `POST /v0/sensitivity/find` endpoint with ACL filtering.
2. Implement optional `POST /v0/search/chunks`.
3. Build minimal React UI with dashboard and findings table.

### Phase 7 – SMB Agent

1. Create separate repo/service for SMB connector.
2. Implement:

   * Config loading from YAML/env.
   * SMB mount assumptions (agent runs in environment where mount already exists or can execute `mount.cifs`).
   * Directory scanning, hashing, ACL stub collection.
   * Event batching and POSTing to `/v0/ingest/events`.
3. Add a basic scheduler to rescan at `scan_interval_seconds`.
